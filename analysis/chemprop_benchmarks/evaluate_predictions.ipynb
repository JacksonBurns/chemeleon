{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7341a5d0",
   "metadata": {},
   "source": [
    "# Imports and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ef2859e-eee6-4745-a7fe-78b1de5575de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "from typing import List, Callable, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (average_precision_score, mean_absolute_error, root_mean_squared_error,\n",
    "                             precision_recall_curve, r2_score, roc_auc_score, mean_absolute_percentage_error, auc)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import ttest_rel\n",
    "from chemprop import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a71ccb53-2e89-4162-b1df-e74660bc563d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_indices(idxs):\n",
    "    \"\"\"Parses a string of indices into a list of integers. e.g. '0,1,2-4' -> [0, 1, 2, 3, 4]\"\"\"\n",
    "    if isinstance(idxs, str):\n",
    "        indices = []\n",
    "        for idx in idxs.split(\",\"):\n",
    "            if \"-\" in idx:\n",
    "                start, end = map(int, idx.split(\"-\"))\n",
    "                indices.extend(range(start, end + 1))\n",
    "            else:\n",
    "                indices.append(int(idx))\n",
    "        return indices\n",
    "    return idxs\n",
    "\n",
    "def prc_auc(targets: List[int], preds: List[float]) -> float:\n",
    "    \"\"\"\n",
    "    Computes the area under the precision-recall curve.\n",
    "\n",
    "    :param targets: A list of binary targets.\n",
    "    :param preds: A list of prediction probabilities.\n",
    "    :return: The computed prc-auc.\n",
    "    \"\"\"\n",
    "    precision, recall, _ = precision_recall_curve(targets, preds)\n",
    "    return auc(recall, precision)\n",
    "\n",
    "def get_metric_func(metric: str):\n",
    "    r\"\"\"\n",
    "    Gets the metric function corresponding to a given metric name.\n",
    "\n",
    "    Supports:\n",
    "\n",
    "    * :code:`roc-auc`: Area under the receiver operating characteristic curve\n",
    "    * :code:`prc-auc`: Area under the precision recall curve\n",
    "    * :code:`ap`: Average precision from prediction scores\n",
    "    * :code:`rmse`: Root mean squared error\n",
    "    * :code:`mae`: Mean absolute error\n",
    "    * :code:`r2`: Coefficient of determination R\\ :superscript:`2`\n",
    "\n",
    "    :param metric: Metric name.\n",
    "    :return: A metric function which takes as arguments a list of targets and a list of predictions and returns.\n",
    "    \"\"\"\n",
    "    if metric == 'roc-auc':\n",
    "        return roc_auc_score\n",
    "\n",
    "    if metric == 'prc-auc':\n",
    "        return prc_auc\n",
    "    \n",
    "    if metric == 'ap':\n",
    "        return average_precision_score\n",
    "\n",
    "    if metric == 'rmse':\n",
    "        return root_mean_squared_error\n",
    "    \n",
    "    if metric == 'mae':\n",
    "        return mean_absolute_error\n",
    "\n",
    "    if metric == 'r2':\n",
    "        return r2_score\n",
    "    \n",
    "    raise ValueError(f'Metric \"{metric}\" not supported.')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb016198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_results(data_path, splits_path, result_dir, num_tasks, metrics, target_columns=None):\n",
    "    df = pd.read_csv(data_path)\n",
    "    with open(splits_path, \"rb\") as json_file:\n",
    "        split_idxss = json.load(json_file)\n",
    "    test_indices = [parse_indices(d[\"test\"]) for d in split_idxss]\n",
    "    test_df = df.iloc[test_indices[0]]\n",
    "    target_columns=test_df.keys()[-num_tasks:].tolist() if target_columns is None else target_columns\n",
    "\n",
    "    df_pred_list = []\n",
    "    files = glob.glob(os.path.join(result_dir, '**', \"test_predictions.csv\"), recursive=True)\n",
    "    assert len(files) == 5, f\"There should be 5 files; {len(files)} found\"\n",
    "    for file in files:\n",
    "        df_pred = pd.read_csv(file)[target_columns]\n",
    "        df_pred_list.append(df_pred)\n",
    "    df_pred = pd.concat(df_pred_list).groupby(level=0).mean()\n",
    "\n",
    "    metric_to_func = {metric: get_metric_func(metric) for metric in metrics}\n",
    "\n",
    "    results = defaultdict(list)\n",
    "    for column in target_columns:\n",
    "        for metric, metric_func in metric_to_func.items():\n",
    "            preds = df_pred[column].tolist()\n",
    "            targets = test_df[column].tolist()\n",
    "            results[metric].append(metric_func(targets, preds))\n",
    "    results = dict(results)\n",
    "\n",
    "    results_df = pd.DataFrame(results, index=target_columns)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1743afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "def evaluate_results_err(data_path, splits_path, result_dir, num_tasks, metrics, target_columns=None):\n",
    "    # Load data and test split\n",
    "    df = pd.read_csv(data_path)\n",
    "    with open(splits_path, \"rb\") as json_file:\n",
    "        split_idxss = json.load(json_file)\n",
    "    test_indices = [parse_indices(d[\"test\"]) for d in split_idxss]\n",
    "    test_df = df.iloc[test_indices[0]]\n",
    "    \n",
    "    # Determine target columns\n",
    "    target_columns = test_df.keys()[-num_tasks:].tolist() if target_columns is None else target_columns\n",
    "\n",
    "    # Gather prediction files\n",
    "    files = glob.glob(os.path.join(result_dir, '**', \"test_predictions.csv\"), recursive=True)\n",
    "    assert len(files) == 5, f\"There should be 5 files; {len(files)} found\"\n",
    "\n",
    "    # Create a dictionary to store results for each file\n",
    "    results = defaultdict(lambda: defaultdict(list))  # Format: results[file][metric] = [metric values for each target]\n",
    "    \n",
    "    # Map metric names to their respective functions\n",
    "    metric_to_func = {metric: get_metric_func(metric) for metric in metrics}\n",
    "\n",
    "    # Process each prediction file individually\n",
    "    for file in files:\n",
    "        df_pred = pd.read_csv(file)[target_columns]\n",
    "\n",
    "        # For each target column and metric, calculate metrics for the current file\n",
    "        for column in target_columns:\n",
    "            preds = df_pred[column].tolist()\n",
    "            targets = test_df[column].tolist()\n",
    "            for metric, metric_func in metric_to_func.items():\n",
    "                metric_value = metric_func(targets, preds)\n",
    "                results[file][metric].append(metric_value)  # Store the metric value for this file\n",
    "\n",
    "    # Create a dictionary to store aggregated results\n",
    "    aggregated_results = defaultdict(dict)\n",
    "\n",
    "    # Calculate average and standard deviation for each metric across files\n",
    "    for metric in metrics:\n",
    "        metric_values = []\n",
    "\n",
    "        # Gather all metric values for this metric across files\n",
    "        for file in files:\n",
    "            metric_values.append(results[file][metric])\n",
    "\n",
    "        # Convert to a numpy array for easier mean and std calculations\n",
    "        metric_values = np.array(metric_values)\n",
    "\n",
    "        # Store mean and std\n",
    "        aggregated_results[metric][\"mean\"] = np.mean(metric_values, axis=0)\n",
    "        aggregated_results[metric][\"std\"] = np.std(metric_values, axis=0)\n",
    "\n",
    "    # Convert results to DataFrame format\n",
    "    individual_results_df = pd.DataFrame(results)\n",
    "    aggregated_results_df = pd.DataFrame(aggregated_results)\n",
    "\n",
    "    return aggregated_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adfdc914",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_results_1(data_path, splits_path, result_dir, num_tasks, metrics, target_columns=None):\n",
    "    df = pd.read_csv(data_path)\n",
    "    with open(splits_path, \"rb\") as json_file:\n",
    "        split_idxss = json.load(json_file)\n",
    "    test_indices = [parse_indices(d[\"test\"]) for d in split_idxss]\n",
    "    test_df = df.iloc[test_indices[0]]\n",
    "    target_columns=test_df.keys()[-num_tasks:].tolist() if target_columns is None else target_columns\n",
    "\n",
    "    df_pred_list = []\n",
    "    files = glob.glob(os.path.join(result_dir, '**', \"test_predictions.csv\"), recursive=True)\n",
    "    # assert len(files) == 5, f\"There should be 5 files; {len(files)} found\"\n",
    "    for file in files:\n",
    "        df_pred = pd.read_csv(file)[target_columns]\n",
    "        df_pred_list.append(df_pred)\n",
    "    df_pred = pd.concat(df_pred_list).groupby(level=0).mean()\n",
    "\n",
    "    metric_to_func = {metric: get_metric_func(metric) for metric in metrics}\n",
    "\n",
    "    results = defaultdict(list)\n",
    "    for column in target_columns:\n",
    "        for metric, metric_func in metric_to_func.items():\n",
    "            preds = df_pred[column].tolist()\n",
    "            targets = test_df[column].tolist()\n",
    "            results[metric].append(metric_func(targets, preds))\n",
    "    results = dict(results)\n",
    "\n",
    "    results_df = pd.DataFrame(results, index=target_columns)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5bf65e6-bee1-4bef-951a-cf79076d6741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_results_full(data_path, splits_path, result_dir, num_tasks, metrics, target_columns=None, model_path=None):\n",
    "    df = pd.read_csv(data_path)\n",
    "    with open(splits_path, \"rb\") as json_file:\n",
    "        split_idxss = json.load(json_file)\n",
    "    test_indices = [parse_indices(d[\"test\"]) for d in split_idxss]\n",
    "    test_df = df.iloc[test_indices[0]]\n",
    "    target_columns=test_df.keys()[-num_tasks:].tolist() if target_columns is None else target_columns\n",
    "\n",
    "    df_pred_list = []\n",
    "    files = glob.glob(os.path.join(result_dir, '**', \"test_predictions.csv\"), recursive=True)\n",
    "    assert len(files) == 5, f\"There should be 5 files; {len(files)} found\"\n",
    "    for file in files:\n",
    "        df_pred = pd.read_csv(file)[target_columns]\n",
    "        df_pred_list.append(df_pred)\n",
    "    df_pred = pd.concat(df_pred_list).groupby(level=0).mean()\n",
    "    # display(df_pred)\n",
    "    # display(test_df)\n",
    "    metric_to_func = {metric: get_metric_func(metric) for metric in metrics}\n",
    "    \n",
    "    i = 0\n",
    "    test_preds = pd.DataFrame()\n",
    "    for column in target_columns:\n",
    "        test_preds[f'true_{i}'] = test_df[column]\n",
    "        test_preds[f'predicted_{i}'] = df_pred[column]\n",
    "        i=i+1\n",
    "    # display(test_preds)\n",
    "    results = defaultdict(list)\n",
    "    for column in target_columns:\n",
    "        for metric, metric_func in metric_to_func.items():\n",
    "            preds = df_pred[column].tolist()\n",
    "            targets = test_df[column].tolist()\n",
    "            results[metric].append(metric_func(targets, preds))\n",
    "    results = dict(results)\n",
    "\n",
    "    results_df = pd.DataFrame(results, index=target_columns)\n",
    "    \n",
    "    if model_path is not None:\n",
    "        model = models.MPNN.load_from_file(model_path)\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        results_df[\"model_size\"] = total_params\n",
    "    \n",
    "    return results_df, test_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52be7b97",
   "metadata": {},
   "source": [
    "# QM 9 Gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84205daa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mae</th>\n",
       "      <th>rmse</th>\n",
       "      <th>r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gap</th>\n",
       "      <td>0.003219</td>\n",
       "      <td>0.005877</td>\n",
       "      <td>0.98454</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          mae      rmse       r2\n",
       "gap  0.003219  0.005877  0.98454"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"/home/akshatz/bond_order_free/qm9/dataset/qm9_data_filtered.csv\"\n",
    "splits_path = \"/home/akshatz/bond_order_free/qm9/dataset/splits_filtered.json\"\n",
    "result_dir = \"/home/akshatz/benchmarks_chemeleon/qm9/qm9_gap/results\"\n",
    "num_tasks = 1\n",
    "metrics = [\"mae\",\"rmse\", \"r2\"]\n",
    "evaluate_results(data_path, splits_path, result_dir, num_tasks, metrics, target_columns=[\"gap\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1a7b13",
   "metadata": {},
   "source": [
    "# QM 9 U0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c52ce4a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mae</th>\n",
       "      <th>rmse</th>\n",
       "      <th>r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>u0_atom</th>\n",
       "      <td>2.167752</td>\n",
       "      <td>14.132761</td>\n",
       "      <td>0.996425</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              mae       rmse        r2\n",
       "u0_atom  2.167752  14.132761  0.996425"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"/home/akshatz/bond_order_free/qm9/dataset/qm9_data_filtered.csv\"\n",
    "splits_path = \"/home/akshatz/bond_order_free/qm9/dataset/splits_filtered.json\"\n",
    "result_dir = \"/home/akshatz/benchmarks_chemeleon/qm9/qm9_u0/results\"\n",
    "num_tasks = 1\n",
    "metrics = [\"mae\", \"rmse\", \"r2\"]\n",
    "evaluate_results(data_path, splits_path, result_dir, num_tasks, metrics, target_columns=[\"u0_atom\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683dd9dc",
   "metadata": {},
   "source": [
    "# QM 9 Multitask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a800bb95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mae</th>\n",
       "      <th>rmse</th>\n",
       "      <th>r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mu</th>\n",
       "      <td>0.347369</td>\n",
       "      <td>0.607233</td>\n",
       "      <td>0.842723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha</th>\n",
       "      <td>0.294276</td>\n",
       "      <td>0.824264</td>\n",
       "      <td>0.989407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>homo</th>\n",
       "      <td>0.002494</td>\n",
       "      <td>0.004263</td>\n",
       "      <td>0.962277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lumo</th>\n",
       "      <td>0.002470</td>\n",
       "      <td>0.004171</td>\n",
       "      <td>0.992052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gap</th>\n",
       "      <td>0.003377</td>\n",
       "      <td>0.005998</td>\n",
       "      <td>0.983895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r2</th>\n",
       "      <td>18.115125</td>\n",
       "      <td>36.033951</td>\n",
       "      <td>0.983356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zpve</th>\n",
       "      <td>0.000463</td>\n",
       "      <td>0.001533</td>\n",
       "      <td>0.997844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv</th>\n",
       "      <td>0.134403</td>\n",
       "      <td>0.330809</td>\n",
       "      <td>0.993252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>u0_atom</th>\n",
       "      <td>3.727032</td>\n",
       "      <td>15.641594</td>\n",
       "      <td>0.995621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>u298_atom</th>\n",
       "      <td>3.754351</td>\n",
       "      <td>15.748834</td>\n",
       "      <td>0.995639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h298_atom</th>\n",
       "      <td>3.772095</td>\n",
       "      <td>15.842080</td>\n",
       "      <td>0.995649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g298_atom</th>\n",
       "      <td>3.529409</td>\n",
       "      <td>14.506117</td>\n",
       "      <td>0.995548</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 mae       rmse        r2\n",
       "mu          0.347369   0.607233  0.842723\n",
       "alpha       0.294276   0.824264  0.989407\n",
       "homo        0.002494   0.004263  0.962277\n",
       "lumo        0.002470   0.004171  0.992052\n",
       "gap         0.003377   0.005998  0.983895\n",
       "r2         18.115125  36.033951  0.983356\n",
       "zpve        0.000463   0.001533  0.997844\n",
       "cv          0.134403   0.330809  0.993252\n",
       "u0_atom     3.727032  15.641594  0.995621\n",
       "u298_atom   3.754351  15.748834  0.995639\n",
       "h298_atom   3.772095  15.842080  0.995649\n",
       "g298_atom   3.529409  14.506117  0.995548"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"/home/akshatz/bond_order_free/qm9/dataset/qm9_data_filtered.csv\"\n",
    "splits_path = \"/home/akshatz/bond_order_free/qm9/dataset/splits_filtered.json\"\n",
    "result_dir = \"/home/akshatz/benchmarks_chemeleon/qm9/qm9_multitask/results\"\n",
    "num_tasks = 12\n",
    "metrics = [\"mae\", \"rmse\", \"r2\"]\n",
    "evaluate_results(data_path, splits_path, result_dir, num_tasks, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a1d7f6",
   "metadata": {},
   "source": [
    "# Multi Molecule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1550a0f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mae</th>\n",
       "      <th>rmse</th>\n",
       "      <th>r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>peakwavs_max</th>\n",
       "      <td>16.859979</td>\n",
       "      <td>31.363678</td>\n",
       "      <td>0.910984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    mae       rmse        r2\n",
       "peakwavs_max  16.859979  31.363678  0.910984"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"/home/akshatz/bond_order_free/multi_molecule/dataset/mult_mol_data_filtered.csv\"\n",
    "splits_path = \"/home/akshatz/bond_order_free/multi_molecule/dataset/splits_filtered.json\"\n",
    "result_dir = \"/home/akshatz/benchmarks_chemeleon/uv_vis/results\"\n",
    "num_tasks = 1\n",
    "metrics = [\"mae\", \"rmse\", \"r2\"]\n",
    "evaluate_results(data_path, splits_path, result_dir, num_tasks, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7670b27e",
   "metadata": {},
   "source": [
    "# HIV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "074a1ef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roc-auc</th>\n",
       "      <th>ap</th>\n",
       "      <th>prc-auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>HIV_active</th>\n",
       "      <td>0.768079</td>\n",
       "      <td>0.336912</td>\n",
       "      <td>0.331843</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             roc-auc        ap   prc-auc\n",
       "HIV_active  0.768079  0.336912  0.331843"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_path = \"/home/akshatz/bond_order_free/hiv/dataset/hiv_data_filtered.csv\"\n",
    "splits_path = \"/home/akshatz/bond_order_free/hiv/dataset/splits_filtered.json\"\n",
    "result_dir = \"/home/akshatz/benchmarks_chemeleon/hiv/results\"\n",
    "num_tasks = 1\n",
    "metrics = [\"roc-auc\", \"ap\", \"prc-auc\"]\n",
    "results = evaluate_results(data_path, splits_path, result_dir, num_tasks, metrics, target_columns=[\"HIV_active\"])\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8294b912",
   "metadata": {},
   "source": [
    "# PCBA Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b30291c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_results_with_nan_targets(data_path, splits_path, result_dir, num_tasks, metrics, target_columns=None):\n",
    "    df = pd.read_csv(data_path)\n",
    "    with open(splits_path, \"rb\") as json_file:\n",
    "        split_idxss = json.load(json_file)\n",
    "    test_indices = [parse_indices(d[\"test\"]) for d in split_idxss]\n",
    "    test_df = df.iloc[test_indices[0]]\n",
    "    target_columns=test_df.keys()[-num_tasks:].tolist() if target_columns is None else target_columns\n",
    "\n",
    "    df_pred_list = []\n",
    "    files = glob.glob(os.path.join(result_dir, '**', \"test_predictions.csv\"), recursive=True)\n",
    "    assert len(files) == 5, f\"There should be 5 files; {len(files)} found\"\n",
    "    for file in files:\n",
    "        df_pred = pd.read_csv(file)[target_columns]\n",
    "        df_pred_list.append(df_pred)\n",
    "    df_pred = pd.concat(df_pred_list).groupby(level=0).mean()\n",
    "\n",
    "    metric_to_func = {metric: get_metric_func(metric) for metric in metrics}\n",
    "\n",
    "    results = defaultdict(list)\n",
    "    for column in target_columns:\n",
    "        for metric, metric_func in metric_to_func.items():\n",
    "            preds = df_pred[column].tolist()\n",
    "            # targets = test_df[column].fillna(0.0).tolist()\n",
    "            targets = test_df[column].tolist()\n",
    "            \n",
    "            pairs = [(target, pred) for target, pred in zip(targets, preds) if not np.isnan(target)]\n",
    "            targets = [item[0] for item in pairs]\n",
    "            preds = [item[1] for item in pairs]\n",
    "            \n",
    "            results[metric].append(metric_func(targets, preds))\n",
    "    results = dict(results)\n",
    "\n",
    "    results_df = pd.DataFrame(results, index=target_columns)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bc2bd66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prc-auc    0.210400\n",
       "ap         0.215105\n",
       "roc-auc    0.909773\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"/home/akshatz/bond_order_free/pcba_random/dataset/pcba_random_data_filtered.csv\"\n",
    "splits_path = \"/home/akshatz/bond_order_free/pcba_random/dataset/splits_filtered.json\"\n",
    "result_dir = \"/home/akshatz/benchmarks_chemeleon/pcba/pcba_random/results\"\n",
    "num_tasks = 128\n",
    "metrics = [\"prc-auc\", \"ap\", \"roc-auc\"]\n",
    "evaluate_results_with_nan_targets(data_path, splits_path, result_dir, num_tasks, metrics).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effa794a",
   "metadata": {},
   "source": [
    "# PCBA Random NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbfe9966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prc-auc    0.369269\n",
       "ap         0.375512\n",
       "roc-auc    0.905497\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"/home/akshatz/bond_order_free/pcba_random_nan/dataset/pcba_random_nan_data_filtered.csv\"\n",
    "splits_path = \"/home/akshatz/bond_order_free/pcba_random_nan/dataset/splits_filtered.json\"\n",
    "result_dir = \"/home/akshatz/benchmarks_chemeleon/pcba/pcba_random_nan/results\"\n",
    "num_tasks = 128\n",
    "metrics = [\"prc-auc\", \"ap\", \"roc-auc\"]\n",
    "evaluate_results_with_nan_targets(data_path, splits_path, result_dir, num_tasks, metrics).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bcb9ad",
   "metadata": {},
   "source": [
    "# PCBA Scaffold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40b33703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prc-auc    0.287534\n",
       "ap         0.291340\n",
       "roc-auc    0.889468\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"/home/akshatz/bond_order_free/pcba_scaffold/dataset/pcba_scaffold_data_filtered.csv\"  # setting NaN targets to 0\n",
    "splits_path = \"/home/akshatz/bond_order_free/pcba_scaffold/dataset/splits_filtered.json\"\n",
    "result_dir = \"/home/akshatz/benchmarks_chemeleon/pcba/pcba_scaffold/results\"\n",
    "num_tasks = 127\n",
    "metrics = [\"prc-auc\", \"ap\", \"roc-auc\"]\n",
    "evaluate_results_with_nan_targets(data_path, splits_path, result_dir, num_tasks, metrics).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d3f027",
   "metadata": {},
   "source": [
    "# PCQM4MV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f27a392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mae</th>\n",
       "      <th>rmse</th>\n",
       "      <th>r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>homolumogap</th>\n",
       "      <td>0.092909</td>\n",
       "      <td>0.154394</td>\n",
       "      <td>0.982413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  mae      rmse        r2\n",
       "homolumogap  0.092909  0.154394  0.982413"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"/home/akshatz/bond_order_free/pcqm4mv2/dataset/pcqm4mv2_data_filtered.csv\"\n",
    "splits_path = \"/home/akshatz/bond_order_free/pcqm4mv2/dataset/splits_filtered.json\"\n",
    "result_dir = \"/home/akshatz/benchmarks_chemeleon/pcqm4mv2/results\"\n",
    "num_tasks = 1\n",
    "metrics = [\"mae\", \"rmse\", \"r2\"]\n",
    "evaluate_results(data_path, splits_path, result_dir, num_tasks, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe37f10",
   "metadata": {},
   "source": [
    "# SAMPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d84e1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sampl(test_no, result_dir, metrics):\n",
    "    targets = None\n",
    "    df_pred_list = []\n",
    "    files = glob.glob(os.path.join(result_dir, '**', f\"pred_SAMPL{test_no}.csv\"), recursive=True)\n",
    "    assert len(files) == 1, f\"There should be 1 file; {len(files)} found\"\n",
    "    for file in files:\n",
    "        df = pd.read_csv(file)\n",
    "        \n",
    "        if targets is None:\n",
    "            targets = df[\"logP mean\"].tolist() if test_no != 9 else df[\"new_logPexp_reviewed\"].tolist()\n",
    "            \n",
    "        df_pred = df[\"logP\"]\n",
    "        df_pred_list.append(df_pred)\n",
    "    preds = pd.concat(df_pred_list).groupby(level=0).mean().tolist()\n",
    "    \n",
    "    metric_to_func = {metric: get_metric_func(metric) for metric in metrics}\n",
    "\n",
    "    results = defaultdict(list)\n",
    "    for metric, metric_func in metric_to_func.items():\n",
    "        results[metric].append(metric_func(targets, preds))\n",
    "    results = dict(results)\n",
    "\n",
    "    results_df = pd.DataFrame(results, index=[f\"logP - SAMPL{test_no}\"])\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab94ee2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mae</th>\n",
       "      <th>rmse</th>\n",
       "      <th>r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>logP - SAMPL6</th>\n",
       "      <td>0.267726</td>\n",
       "      <td>0.289255</td>\n",
       "      <td>0.811672</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    mae      rmse        r2\n",
       "logP - SAMPL6  0.267726  0.289255  0.811672"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = metrics = [\"mae\", \"rmse\", \"r2\"]\n",
    "evaluate_sampl(6, \"/home/akshatz/benchmarks_chemeleon/SAMPL/results_sampl_production\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "414d363a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mae</th>\n",
       "      <th>rmse</th>\n",
       "      <th>r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>logP - SAMPL7</th>\n",
       "      <td>0.428742</td>\n",
       "      <td>0.620399</td>\n",
       "      <td>0.127307</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    mae      rmse        r2\n",
       "logP - SAMPL7  0.428742  0.620399  0.127307"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = metrics = [\"mae\", \"rmse\", \"r2\"]\n",
    "evaluate_sampl(7, \"/home/akshatz/benchmarks_chemeleon/SAMPL/results_sampl_production\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4b49527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mae</th>\n",
       "      <th>rmse</th>\n",
       "      <th>r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>logP - SAMPL9</th>\n",
       "      <td>0.806533</td>\n",
       "      <td>1.005415</td>\n",
       "      <td>0.798624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    mae      rmse        r2\n",
       "logP - SAMPL9  0.806533  1.005415  0.798624"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = metrics = [\"mae\", \"rmse\", \"r2\"]\n",
    "evaluate_sampl(9, \"/home/akshatz/benchmarks_chemeleon/SAMPL/results_sampl_production\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa2d633",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemprop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
